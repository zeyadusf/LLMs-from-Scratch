<div align='center'>
  
# LLMs from Scratch
Build a Large Language Model (From Scratch)  **[Chapter 3]**<br>
_Coding Attention Mechanisms_
</div>

**This chapter covers**
* Exploring the reasons for using attention mechanisms in neural networks.
* Introducing a basic self-attention framework and progressing to an enhanced self-attention mechanism.
  > [simple self attention without trainable weights.ipynb](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/03-Coding%20attention%20mechanisms/simple%20self%20attention%20without%20trainable%20weights.ipynb)<br>
  > [attention weigths step by step.ipynb](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/03-Coding%20attention%20mechanisms/attention%20weigths%20step%20by%20step.ipynb)<br>
  > [selfAttention.py](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/03-Coding%20attention%20mechanisms/selfAttention.py)
 
* Implementing a causal attention module that allows LLMs to generate one token at a time
  > [hiding future words with causal attention.ipynb](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/03-Coding%20attention%20mechanisms/hiding%20future%20words%20with%20causal%20attention.ipynb)<br>
  > [causalAttention.py](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/03-Coding%20attention%20mechanisms/causalAttention.py)<br>
* Masking randomly selected attention weights with dropout to reduce overfitting
* Stacking multiple causal attention modules into a multi-head attention module
  > [Stacking multiple single-head attention layers.ipynb](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/03-Coding%20attention%20mechanisms/Stacking%20multiple%20single-head%20attention%20layers.ipynb)<br>
  > [MultiHeadAttention.ipynb](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/03-Coding%20attention%20mechanisms/MultiHeadAttention.py)

8 sep
