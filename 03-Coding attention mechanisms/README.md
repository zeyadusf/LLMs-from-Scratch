<div align='center'>
  
# LLMs from Scratch
Build a Large Language Model (From Scratch)  **[Chapter 3]**<br>
_Coding Attention Mechanisms_
</div>

**This chapter covers**
* Exploring the reasons for using attention mechanisms in neural networks.
* Introducing a basic self-attention framework and progressing to an enhanced self-attention mechanism.
* Implementing a causal attention module that allows LLMs to generate one token at a time
* Masking randomly selected attention weights with dropout to reduce overfitting
* Stacking multiple causal attention modules into a multi-head attention module
