<div align='center'>
  
# LLMs from Scratch
Build a Large Language Model (From Scratch)  **[Chapter 2]**
</div>


**This chapter covers**

* Preparing text for large language model training
* Splitting text into word and subword tokens
  > 2.2 [Processed.py](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/ch02-Working%20with%20Text%20Data/Processed.py)<br>
  > 2.3 & 2.4 [Tokenizer.py](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/ch02-Working%20with%20Text%20Data/Tokenizer.py)<br>
* Byte pair encoding as a more advanced way of tokenizing text
  > 2.5 [Byte pair encoding.ipynb](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/ch02-Working%20with%20Text%20Data/Byte%20pair%20encoding.ipynb)<br>
* Sampling training examples with a sliding window approach
  > 2.6 [SlidingWindow.py](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/ch02-Working%20with%20Text%20Data/slidingWindow.py)<br>
  > 2.6 [Data sampling with sliding window.ipynb](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/ch02-Working%20with%20Text%20Data/Data%20sampling%20with%20sliding%20window.ipynb)<br>
* Converting tokens into vectors that feed into a large language model
  > 2.7 [Creating token embedding.ipynb](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/ch02-Working%20with%20Text%20Data/Creating%20token%20embedding.ipynb)<br>
  > 2.7 [Encoding word position.ipynb](https://github.com/zeyadusf/LLMs-from-Scratch/blob/main/ch02-Working%20with%20Text%20Data/Encoding%20word%20position.ipynb)<br>

  _3 Aug_
